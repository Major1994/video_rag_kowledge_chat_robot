# inverted_index_with_stopwords.py

import jieba
from collections import defaultdict

# ================================
# 1. ç¤ºä¾‹æ–‡æ¡£æ•°æ®
# ================================
documents = {
    "doc1": "çŒ«å–œæ¬¢ç©è€ï¼Œç‰¹åˆ«çˆ±æŠ“è€é¼ ",
    "doc2": "ç‹—å–œæ¬¢è·‘æ­¥ï¼Œéå¸¸å¿ è¯š",
    "doc3": "çŒ«å’Œç‹—éƒ½æ˜¯å¯çˆ±çš„å® ç‰©",
    "doc4": "å¤§å®¶éƒ½çˆ±å¯çˆ±çš„åŠ¨ç‰©"
}

# ================================
# 2. åœç”¨è¯è¡¨ï¼ˆå¯æ‰©å±•ï¼‰
# ================================
# å¸¸è§ä¸­æ–‡åœç”¨è¯
stop_words = {
    'çš„', 'äº†', 'å’Œ', 'éƒ½', 'æ˜¯', 'åœ¨', 'æœ‰', 'è¿™', 'é‚£', 'å°±',
    'ä¹Ÿ', 'å¾ˆ', 'å¤ª', 'åˆ', 'ä¸', 'ä½†', 'åª', 'éƒ½', 'è¦', 'ä¼š',
    'å¯ä»¥', 'èƒ½å¤Ÿ', 'åº”è¯¥', 'ä¸€ä¸ª', 'ä¸€äº›', 'è¿™ç§', 'é‚£ç§',
    'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹', 'ä»–', 'å®ƒ', 'æˆ‘', 'ä½ ',
    'ç€', 'è¿‡', 'å¾—', 'åœ°', 'å¾ˆ', 'å—', 'å‘¢', 'å§', 'å•Š', 'ï¼Œ'
}

# å¯é€‰ï¼šä»æ–‡ä»¶åŠ è½½åœç”¨è¯
# with open("stopwords.txt", encoding="utf-8") as f:
#     stop_words = set(line.strip() for line in f if line.strip())

# ================================
# 3. æ„å»ºå€’æ’ç´¢å¼•ï¼ˆå¸¦åœç”¨è¯è¿‡æ»¤ï¼‰
# ================================
inverted_index = defaultdict(set)

print("ğŸ” åˆ†è¯è¯¦æƒ…ï¼ˆè¿‡æ»¤åï¼‰ï¼š")
for doc_id, content in documents.items():
    # ä½¿ç”¨ jieba.cut_for_searchï¼šæ›´ç»†ç²’åº¦åˆ†è¯
    words = list(jieba.cut_for_search(content))
    filtered_words = []
    for word in words:
        word = word.strip()
        if len(word) == 0 or word in stop_words:
            continue  # è·³è¿‡ç©ºä¸²å’Œåœç”¨è¯
        filtered_words.append(word)
        inverted_index[word].add(doc_id)
    
    print(f"{doc_id}: {filtered_words}")

print("\nğŸ“Š å€’æ’ç´¢å¼•ç»“æ„ï¼ˆå·²è¿‡æ»¤åœç”¨è¯ï¼‰ï¼š")
for word, doc_ids in sorted(inverted_index.items()):
    print(f"  '{word}' â†’ {sorted(doc_ids)}")

# ================================
# 4. æŸ¥è¯¢å‡½æ•°ï¼ˆæŸ¥è¯¢æ—¶ä¹Ÿè¿‡æ»¤åœç”¨è¯ï¼‰
# ================================
def search_and(query_text, index, stop_words_set=None):
    """AND æŸ¥è¯¢ï¼šè¿”å›åŒæ—¶åŒ…å«æ‰€æœ‰è¯çš„æ–‡æ¡£ï¼ˆè¿‡æ»¤åœç”¨è¯ï¼‰"""
    if stop_words_set is None:
        stop_words_set = set()
    words = list(jieba.cut_for_search(query_text))
    result_docs = None
    valid_words = []  # è®°å½•å®é™…ç”¨äºæŸ¥è¯¢çš„è¯
    for word in words:
        word = word.strip()
        if not word or word in stop_words_set:
            continue
        valid_words.append(word)
        docs = index.get(word, set())
        if result_docs is None:
            result_docs = docs
        else:
            result_docs = result_docs & docs
    print(f"ğŸ” AND æŸ¥è¯¢è¯: {valid_words}")
    return result_docs or set()

def search_or(query_text, index, stop_words_set=None):
    """OR æŸ¥è¯¢ï¼šè¿”å›åŒ…å«ä»»æ„ä¸€ä¸ªè¯çš„æ–‡æ¡£"""
    if stop_words_set is None:
        stop_words_set = set()
    words = list(jieba.cut_for_search(query_text))
    result_docs = set()
    valid_words = []
    for word in words:
        word = word.strip()
        if not word or word in stop_words_set:
            continue
        valid_words.append(word)
        result_docs |= index.get(word, set())
    print(f"ğŸ” OR æŸ¥è¯¢è¯: {valid_words}")
    return result_docs

# ================================
# 5. æµ‹è¯•æŸ¥è¯¢
# ================================
print("\nğŸ” æŸ¥è¯¢æµ‹è¯•ï¼š")

# AND æŸ¥è¯¢
res_and = search_and("çŒ« çˆ±", inverted_index, stop_words)
print(f"åŒæ—¶åŒ…å« 'çŒ«' å’Œ 'çˆ±' çš„æ–‡æ¡£: {sorted(res_and)}")

# OR æŸ¥è¯¢
res_or = search_or("çŒ« çˆ±", inverted_index, stop_words)
print(f"åŒ…å« 'çŒ«' æˆ– 'çˆ±' çš„æ–‡æ¡£: {sorted(res_or)}")

# å¤æ‚æŸ¥è¯¢ï¼ˆåœç”¨è¯è‡ªåŠ¨è¿‡æ»¤ï¼‰
res_hard = search_and("å¯çˆ±çš„ åŠ¨ç‰©", inverted_index, stop_words)
print(f"åŒæ—¶åŒ…å« 'å¯çˆ±' å’Œ 'åŠ¨ç‰©' çš„æ–‡æ¡£: {sorted(res_hard)}")